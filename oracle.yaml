# PIPELINE DEFINITION
# Name: oracle
# Inputs:
#    dataset: str [Default: 'gs://protocell/data/words.npz']
#    epochs: int [Default: 10.0]
#    foo: system.Dataset
#    upload: bool [Default: True]
#    upload_threshold: float [Default: 0.0]
components:
  comp-condition-1:
    dag:
      tasks:
        upload-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-model-oracle
          taskInfo:
            name: upload-model
    inputDefinitions:
      artifacts:
        pipelinechannel--train-model-oracle:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--metrics-Output:
          parameterType: NUMBER_INTEGER
        pipelinechannel--upload_threshold:
          parameterType: NUMBER_DOUBLE
  comp-importer:
    executorLabel: exec-importer
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-metrics:
    executorLabel: exec-metrics
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        edit_distance_histogram:
          artifactType:
            schemaTitle: system.Markdown
            schemaVersion: 0.0.1
        predictions:
          artifactType:
            schemaTitle: system.Markdown
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: NUMBER_INTEGER
  comp-shap-explainer:
    executorLabel: exec-shap-explainer
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        cols:
          parameterType: NUMBER_INTEGER
        rows:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        explanation:
          artifactType:
            schemaTitle: system.Markdown
            schemaVersion: 0.0.1
  comp-split-data:
    executorLabel: exec-split-data
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        ratio:
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        base_output_directory:
          defaultValue: gs://protocell/pipeline
          isOptional: true
          parameterType: STRING
        display_name:
          defaultValue: oracle
          isOptional: true
          parameterType: STRING
        enable_web_access:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        encryption_spec_key_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        epochs:
          parameterType: NUMBER_INTEGER
        labels:
          defaultValue: {}
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          isOptional: true
          parameterType: STRING
        network:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          isOptional: true
          parameterType: STRING
        reserved_ip_ranges:
          defaultValue: []
          isOptional: true
          parameterType: LIST
        restart_job_on_worker_restart:
          defaultValue: false
          isOptional: true
          parameterType: BOOLEAN
        service_account:
          defaultValue: 429426973958-compute@developer.gserviceaccount.com
          isOptional: true
          parameterType: STRING
        tensorboard:
          defaultValue: projects/429426973958/locations/europe-central2/tensorboards/2373397003624251392
          isOptional: true
          parameterType: STRING
        timeout:
          defaultValue: 604800s
          isOptional: true
          parameterType: STRING
        worker_pool_specs:
          defaultValue:
          - container_spec:
              args:
              - --executor_input
              - '{{$.json_escape[1]}}'
              - --function_to_execute
              - train_model
              command:
              - sh
              - -c
              - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip\
                \ || python3 -m ensurepip --user || apt-get install python3-pip\n\
                fi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\
                \ --no-warn-script-location 'kfp==2.4.0' '--no-deps' 'typing-extensions>=3.7.4,<5;\
                \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
              - sh
              - -ec
              - 'program_path=$(mktemp -d)


                printf "%s" "$0" > "$program_path/ephemeral_component.py"

                _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

                '
              - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing\
                \ import *\n\ndef train_model(epochs: int, dataset: Input[Dataset],\
                \ oracle: Output[Model]):\n    import numpy\n    import keras\n  \
                \  import os\n\n    from keras.layers import (\n        Conv2D,\n\
                \        BatchNormalization,\n        MaxPooling2D,\n        Dropout,\n\
                \        Flatten,\n        Dense,\n        Activation,\n        Permute,\n\
                \        Reshape,\n        Bidirectional,\n        LSTM\n    )\n \
                \   from keras import callbacks\n    import tensorflow as tf\n   \
                \ from src import aitoolkit\n\n    image_width = 128\n    image_height\
                \ = 32\n\n    # Character codes\n    characters = [\n        '!',\
                \ '\"', '#', '&', \"'\", '(', ')', '*', '+', ',',\n        '-', '.',\
                \ '/', '0', '1', '2', '3', '4', '5', '6',\n        '7', '8', '9',\
                \ ':', ';', '?', 'A', 'B', 'C', 'D',\n        'E', 'F', 'G', 'H',\
                \ 'I', 'J', 'K', 'L', 'M', 'N',\n        'O', 'P', 'Q', 'R', 'S',\
                \ 'T', 'U', 'V', 'W', 'X',\n        'Y', 'Z', 'a', 'b', 'c', 'd',\
                \ 'e', 'f', 'g', 'h',\n        'i', 'j', 'k', 'l', 'm', 'n', 'o',\
                \ 'p', 'q', 'r',\n        's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n\
                \    ]\n\n    X, Y = numpy.load(dataset.path + \".npz\").values()\n\
                \    maxlen = len(max(Y, key=len))\n    X, Y = aitoolkit.format_data(X,\
                \ Y, characters, maxlen)\n    X_test, Y_test\t = X[:1000], Y[:1000]\n\
                \    X_train, Y_train = X[1000:2000], Y[1000:2000] #todo\n\n\n   \
                \ tensorboard = callbacks.TensorBoard(\n        # vertex-provided\
                \ directory for logs\n        log_dir = os.environ['AIP_TENSORBOARD_LOG_DIR'],\n\
                \        histogram_freq=1\n    )\n    checkpoints = callbacks.ModelCheckpoint(\n\
                \        monitor='edit_distance',\n        filepath='./weights.h5',\n\
                \        save_weights_only=True,\n        save_best_only=True,\n \
                \       mode='min'\n    )\n    plateau = callbacks.ReduceLROnPlateau(\n\
                \        monitor='edit_distance',\n        patience=10,\n        verbose=1,\n\
                \        factor=0.90\n    )\n\n    namespace = tf.Graph()\n\n    def\
                \ Convolutions(filters, kernel=5):\n        return keras.models.Sequential([\n\
                \            Conv2D(filters, kernel, padding='same', kernel_initializer=\"\
                he_normal\"),\n            BatchNormalization(),\n            Activation('relu')\n\
                \        ], name=namespace.unique_name(\"convolutions\"))\n\n\n  \
                \  def build_model():\n        input = keras.Input(shape=(image_height,\
                \ image_width, 1), name=\"images\")\n\n        x = Convolutions(128,\
                \ 3)(input)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Convolutions(64,\
                \ 3)(x)\n        x = MaxPooling2D((2, 2))(x)\n\n        x = Permute([2,\
                \ 1, 3])(x)\n        x = Reshape([image_width // 4, -1])(x)\n\n  \
                \      x = Dense(64, activation=\"relu\")(x)\n        x = Dropout(0.2)(x)\n\
                \n        x = Bidirectional( LSTM(128, return_sequences=True, dropout=0.25)\
                \ )(x)\n        x = Bidirectional( LSTM(64,  return_sequences=True,\
                \ dropout=0.25) )(x)\n\n        # Reserve two extra tokens for the\
                \ ctc_loss\n        x = Dense(len(characters) + 2, activation=\"softmax\"\
                , name=\"predictions\")(x)\n\n        model = keras.models.Model(inputs=input,\
                \ outputs=x, name=\"oracle\")\n        model.compile(optimizer=\"\
                adam\", loss=aitoolkit.ctc_loss, metrics=[aitoolkit.edit_distance])\n\
                \        return model\n\n    model = build_model()\n    print(\"Parameter\
                \ count:\", model.count_params())\n    model.summary()\n\n\n    model.fit(\n\
                \        X_train, Y_train,\n        validation_split = 0.1,\n    \
                \    epochs = epochs,\n        callbacks = [tensorboard, checkpoints,\
                \ plateau]\n    )\n\n    # aitoolkit.save(model, oracle.path)\n  \
                \  model.save(f\"{oracle.path}.keras\")\n\n"
              env: []
              image_uri: europe-central2-docker.pkg.dev/protocell-404013/kubeflow-images/keras:latest
            disk_spec:
              boot_disk_size_gb: 100.0
              boot_disk_type: pd-ssd
            machine_spec:
              machine_type: n1-standard-4
            replica_count: 1.0
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        oracle:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        gcp_resources:
          parameterType: STRING
  comp-upload-model:
    executorLabel: exec-upload-model
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        container_image:
          defaultValue: europe-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.2-15:latest
          isOptional: true
          parameterType: STRING
defaultPipelineRoot: gs://protocell/pipeline
deploymentSpec:
  executors:
    exec-importer:
      importer:
        artifactUri:
          runtimeParameter: uri
        typeSchema:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
    exec-metrics:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - metrics
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'seaborn' &&\
          \ \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef metrics(\n    dataset: Input[Dataset],\n    model: Input[Model],\n\
          \    edit_distance_histogram: Output[Markdown],\n    predictions: Output[Markdown]\n\
          ) -> int:                                                         \n\n \
          \   from src import aitoolkit\n    from src.utils import capture_image\n\
          \    import seaborn\n    import numpy\n\n    X, Y = numpy.load(dataset.path\
          \ + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, aitoolkit.characters)\n\
          \    model = aitoolkit.load(model.path)\n    batch = aitoolkit.batch_prediction(model,\
          \ X, Y)\n\n    edit_distance_values = numpy.array([sample.value for sample\
          \ in batch])\n    seaborn.histplot(edit_distance_values, bins=10, kde=True,\
          \ alpha=0.6)\n    open(edit_distance_histogram.path, 'w').write(f\"![Image]({capture_image()})\"\
          )\n\n    batch.show()\n    open(predictions.path, 'w').write(f\"![Image]({capture_image()})\"\
          )\n\n    average_edit_distance = numpy.mean(edit_distance_values)\n    print(\"\
          Average edit distance:\", average_edit_distance)\n    return int(average_edit_distance)\n\
          \n"
        image: europe-central2-docker.pkg.dev/protocell-404013/kubeflow-images/keras:latest
    exec-shap-explainer:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - shap_explainer
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'shap==0.44'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef shap_explainer(\n    rows: int,\n    cols: int,\n    dataset:\
          \ Input[Dataset],\n    model: Input[Model],\n    explanation: Output[Markdown]\n\
          ):                                                         \n    import\
          \ cv2\n    import shap\n    import numpy\n    import keras\n    import tensorflow\
          \ as tf\n\n    from src import aitoolkit\n    from src.utils import capture_image\n\
          \n    characters = aitoolkit.characters\n    X, Y = numpy.load(dataset.path\
          \ + \".npz\").values()\n    X, Y = aitoolkit.format_data(X, Y, characters)\n\
          \    model = aitoolkit.load(model.path)\n\n    # repurpose model for \"\
          classification\"\n    classifier = keras.models.Model(model.input, tf.reduce_max(model.output,\
          \ axis=1)[:, :len(characters)])\n\n    masker = shap.maskers.Image(\"inpaint_telea\"\
          , X[0].shape)\n    explainer = shap.Explainer(classifier, masker, output_names=list(characters))\n\
          \    indices = numpy.random.randint(len(X), size=rows)\n    shap_values\
          \ = explainer(X[indices], max_evals=100, batch_size=1000, outputs=shap.Explanation.argsort.flip[:cols])\n\
          \n    labels = shap_values.output_names\n    if len(numpy.shape(shap_values.output_names))\
          \ == 1:\n        # shap is an awful library and thus requires awful fixes\n\
          \        labels = numpy.reshape(labels * rows, (rows, cols))\n\n    shap.image_plot(shap_values,\
          \ labels=labels)\n    open(explanation.path, 'w').write(f\"![Image]({capture_image()})\"\
          )\n\n"
        image: europe-central2-docker.pkg.dev/protocell-404013/kubeflow-images/keras:latest
    exec-split-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_data(ratio: float, dataset: Input[Dataset], train: Output[Dataset],\
          \ test: Output[Dataset]):\n    import numpy\n    X, Y = numpy.load(dataset.path).values()\n\
          \    test_size = int(ratio * len(X))\n\n    train_set = (X[test_size:],\
          \ Y[test_size:])\n    test_set  = (X[:test_size], Y[:test_size])\n\n   \
          \ numpy.savez(train.path, *train_set)\n    numpy.savez(test.path, *test_set)\n\
          \n"
        image: europe-central2-docker.pkg.dev/protocell-404013/kubeflow-images/keras:latest
    exec-train-model:
      container:
        args:
        - --type
        - CustomJob
        - --payload
        - '{"display_name": "{{$.inputs.parameters[''display_name'']}}", "job_spec":
          {"worker_pool_specs": {{$.inputs.parameters[''worker_pool_specs'']}}, "scheduling":
          {"timeout": "{{$.inputs.parameters[''timeout'']}}", "restart_job_on_worker_restart":
          {{$.inputs.parameters[''restart_job_on_worker_restart'']}}}, "service_account":
          "{{$.inputs.parameters[''service_account'']}}", "tensorboard": "{{$.inputs.parameters[''tensorboard'']}}",
          "enable_web_access": {{$.inputs.parameters[''enable_web_access'']}}, "network":
          "{{$.inputs.parameters[''network'']}}", "reserved_ip_ranges": {{$.inputs.parameters[''reserved_ip_ranges'']}},
          "base_output_directory": {"output_uri_prefix": "{{$.inputs.parameters[''base_output_directory'']}}"}},
          "labels": {{$.inputs.parameters[''labels'']}}, "encryption_spec": {"kms_key_name":
          "{{$.inputs.parameters[''encryption_spec_key_name'']}}"}}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.custom_job.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.8.0
    exec-upload-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model(\n    model: Input[Model],\n    container_image:\
          \ str = \"europe-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.2-15:latest\"\
          \n):                                                         \n    from\
          \ google.cloud import aiplatform\n    from src import aitoolkit\n    from\
          \ src.secrets import configs\n\n    model = aitoolkit.load(model.path)\n\
          \    model.save(model.path, save_format=\"tf\")\n\n    print(\"uploading\
          \ to model registry\")\n    model = aiplatform.Model.upload(\n        display_name=configs.model,\n\
          \        artifact_uri=model.path,\n        serving_container_image_uri=container_image\n\
          \    )\n\n"
        image: europe-central2-docker.pkg.dev/protocell-404013/kubeflow-images/keras:latest
pipelineInfo:
  name: oracle
root:
  dag:
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - metrics
        - train-model
        inputs:
          artifacts:
            pipelinechannel--train-model-oracle:
              taskOutputArtifact:
                outputArtifactKey: oracle
                producerTask: train-model
          parameters:
            pipelinechannel--metrics-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: metrics
            pipelinechannel--upload_threshold:
              componentInputParameter: upload_threshold
        taskInfo:
          name: metric > threshold
        triggerPolicy:
          condition: double(int(inputs.parameter_values['pipelinechannel--metrics-Output']))
            > inputs.parameter_values['pipelinechannel--upload_threshold']
      importer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-importer
        inputs:
          parameters:
            uri:
              componentInputParameter: dataset
        taskInfo:
          name: importer
      metrics:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-metrics
        dependentTasks:
        - split-data
        - train-model
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: test
                producerTask: split-data
            model:
              taskOutputArtifact:
                outputArtifactKey: oracle
                producerTask: train-model
        taskInfo:
          name: metrics
      shap-explainer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-shap-explainer
        dependentTasks:
        - split-data
        - train-model
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: test
                producerTask: split-data
            model:
              taskOutputArtifact:
                outputArtifactKey: oracle
                producerTask: train-model
          parameters:
            cols:
              runtimeValue:
                constant: 5.0
            rows:
              runtimeValue:
                constant: 4.0
        taskInfo:
          name: shap-explainer
      split-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-data
        dependentTasks:
        - importer
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: artifact
                producerTask: importer
          parameters:
            ratio:
              runtimeValue:
                constant: 0.1
        taskInfo:
          name: split-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - split-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: train
                producerTask: split-data
          parameters:
            epochs:
              componentInputParameter: epochs
            location:
              runtimeValue:
                constant: europe-central2
        taskInfo:
          name: train-model
  inputDefinitions:
    artifacts:
      foo:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
        isOptional: true
    parameters:
      dataset:
        defaultValue: gs://protocell/data/words.npz
        isOptional: true
        parameterType: STRING
      epochs:
        defaultValue: 10.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      upload:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      upload_threshold:
        defaultValue: 0.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.4.0
